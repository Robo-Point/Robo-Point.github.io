<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>RoboPoint</title>
    <meta name="description" content="RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics">
    <meta name="keywords" content="Foundation Model, Affordance Prediction">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <script>
        function updateManipMethodVideo() {
            const method = document.getElementById("single-menu-method-manipulation").value;
            const video = document.getElementById("result-manipulation-video");
            const uri = "static/videos/results/manipulation/" + method + ".mp4"
            video.src = uri;
            video.playbackRate = 1.75;
            video.play();
        }
        function updateNavMethodVideo() {
            const method = document.getElementById("single-menu-method-navigation").value;
            const video = document.getElementById("result-navigation-video");
            const uri = "static/videos/results/navigation/" + method + ".mp4"
            video.src = uri;
            video.playbackRate = 1.75;
            video.play();
        }
    </script>
</head>

<body onload="updateManipMethodVideo(); updateNavMethodVideo();">

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://wentaoyuan.github.io/">Wentao Yuan</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://duanjiafei.com/">Jiafei Duan</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://www.cs.cornell.edu/~valts/">Valts Blukis</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://wpumacay.github.io/">Wilbert Pumacay</a><sup>4</sup>
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="http://ranjaykrishna.com/">Ranjay Krishna</a><sup>1, 3</sup>
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="http://adithyamurali.com/">Adithyavairavan Murali</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>1, 2</sup>
                            </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>University of Washington</span>
                            <span class="author-block"><sup>2</sup>NVIDIA</span>
                            <br>
                            <span class="author-block"><sup>3</sup>Allen Institute for Artifical Intelligence</span>
                            <span class="author-block"><sup>4</sup>Universidad Cat√≥lica San Pablo</span>
                        </div>
                        <div class="column has-text-centered">
                            <!-- ArXiv link -->
                            <span class="link-block">
                                <a target="_blank" href="https://arxiv.org/pdf/2406.10721" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon"><i class="fas fa-file"></i></span>
                                    <span>ArXiv</span>
                                </a>
                            </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                                <a target="_blank" href="https://github.com/wentaoyuan/RoboPoint" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon"><i class="fab fa-github"></i></span>
                                    <span>Code</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser and Abstract -->
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Video Teaser -->
            <video id="teaser" autoplay muted loop height="100">
                <source src="static/videos/vid_intro_teaser.mp4" type="video/mp4">
            </video>
            <!-- /Video Teaser -->
            <br/>
            <br/>
            <!-- Abstract -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            From rearranging objects on a table to putting groceries into shelves,
                            robots must plan precise action points to perform tasks accurately and reliably. In
                            spite of the recent adoption of vision language models (VLMs) to control robot
                            behavior, VLMs struggle to precisely articulate robot actions using language. We
                            introduce an automatic synthetic data generation pipeline that instruction-tunes
                            VLMs to robotic domains and needs. Using the pipeline, we train <span class="drobopoint">ROBOPOINT</span>,
                            a VLM that predicts image keypoint affordances given language instructions. Compared
                            to alternative approaches, our method requires no real-world data collection
                            or human demonstration, making it much more scalable to diverse environments
                            and viewpoints. In addition, <span class="drobopoint">ROBOPOINT</span> is a general
                            model that enables several downstream applications such as robot navigation,
                            manipulation, and augmented reality (AR) assistance. Our experiments demonstrate
                            that <span class="drobopoint">ROBOPOINT</span> outperforms state-of-the-art
                            VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy
                            of predicting spatial affordance and by 30.5% in the success rate of downstream tasks.
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Abstract -->
        </div>
    </section>

    <section>
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">RoboPoint Downstream Applications</h2>
                    <div class="columns is-vcentered interpolation-panel">
                        <div class="column  has-text-centered">
                            <video autoplay controls muted loop playsinline>
                                <source src="static/videos/vid_ar.mp4" type="video/mp4">
                            </video>
                            <p >AR Assistance</p>
                        </div>
                        <div class="column  has-text-centered">
                            <video autoplay controls muted loop playsinline>
                                <source src="static/videos/vid_manipulation.mp4" type="video/mp4">
                            </video>
                            <p >Manipulation</p>
                        </div>
                        <div class="column  has-text-centered">
                            <video autoplay controls muted loop playsinline>
                                <source src="static/videos/vid_navigation.mp4" type="video/mp4">
                            </video>
                            <p >Navigation</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <h2 class="title is-3 has-text-centered">RoboPoint Overview</h2>

                    <figure>
                        <img src="static/images/overview.png" alt="Robot Arm">
                        <figcaption>An RGB image is rendered from a procedurally generated 3D scene. We compute spatial relations from the camera's perspective and generate affordances by sampling points within object masks and object-surface intersections. These instruction-point pairs fine-tune the language model. During deployment, RoboPoint predicts 2D action points from an image and instruction, which are projected into 3D using a depth map. The robot then navigates to these 3D targets with a motion planner.</figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
    <div class="container is-max-widescreen">
        <div class="rows is-centered">
            <h2 class="title is-3 has-text-centered">Dataset for instruction fine-tuning</h2>

            <figure class="has-text-centered">
                <img src="static/images/data.png" alt="Robot Arm">
                <figcaption>We combine object and space reference data with VQA and object detection data. RoboPoint leverages spatial reasoning, object detection, and affordance prediction from these diverse sources, enabling it to generalize combinatorially.</figcaption>
            </figure>
            <figure class="has-text-centered">
                <img src="static/images/c.png" alt="Robot Arm">
                <figcaption>Examples from the synthetic dataset used to teach RoboPoint relational object reference and free space reference. The red and ground boxes are visual prompts to indicate reference objects and the cyan dots are the visualized ground truth (not included in the image inputs to the model).</figcaption>
            </figure>
        </div>
    </div>
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows is-centered">
            <h2 class="title is-3 has-text-centered">Evaluation Results</h2>

            <figure class="has-text-centered">
                <img src="static/images/Evaluation performance.png" alt="Robot Arm">
                <figcaption>Qualitative results on RoboRefIt (a, b) and WHERE2PLACE (c, d, e, f, g, h), including cases with relations unseen during training (d, e, f, h) and where GPT-4o performs better (g, h).</figcaption>
            </figure>
        </div>
    </div>
</section>



    

    <!-- Manipulation method comparison -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3">RoboPoint Application Results</h2>
                        <div class="columns">
                            <div class="column has-text-centered">
                                <h3 class="title is-5">Manipulation Results</h3>
                                Method
                                <div class="select is-small">
                                    <select id="single-menu-method-manipulation" onchange="updateManipMethodVideo()">
                                        <option value="gpt4v" selected="selected">GPT4-V</option>
                                        <option value="pivot">PIVOT</option>
                                        <option value="qwenvl">QwenVL</option>
                                        <option value="robopoint">RoboPoint</option>
                                    </select>
                                </div>
                                <br>
                                <br>
                                <video id="result-manipulation-video" muted autoplay loop>
                                    <source src="static/videos/results/manipulation/gpt4v.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- /Manipulation method comparison -->

    <!-- Navigation method comparison -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <div class="columns">
                            <div class="column has-text-centered">
                                <h3 class="title is-5">Navigation Results</h3>
                                Method
                                <div class="select is-small">
                                    <select id="single-menu-method-navigation" onchange="updateNavMethodVideo()">
                                        <option value="gpt4v" selected="selected">GPT4-V</option>
                                        <option value="pivot">PIVOT</option>
                                        <option value="robopoint">RoboPoint</option>
                                    </select>
                                </div>
                                <br>
                                <br>
                                <video id="result-navigation-video" muted autoplay loop>
                                    <source src="static/videos/results/navigation/gpt4v.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- /Navigation method comparison -->


</body>
</html>
